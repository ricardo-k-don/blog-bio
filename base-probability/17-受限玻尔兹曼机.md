# 受限玻尔兹曼机

玻尔兹曼机是一种存在隐节点的无向图模型。在图模型中最简单的是朴素贝叶斯模型（朴素贝叶斯假设），引入单个隐变量后，发展出了 GMM，若单个隐变量变成序列的隐变量，就得到了状态空间模型（引入齐次 Markov 假设和观测独立假设就有混合概率图），为了引入观测变量之间的关联，引入了一种最大熵模型 MEMM，为了克服 MEMM 中的局域问题，又引入了 CRF，CRF 是一个无向图，其中，破坏了齐次 Markov 假设，若隐变量是一个链式结构，则又叫线性链 CRF。

在无向图的基础上，引入隐变量得到了玻尔兹曼机，这个图模型的概率密度函数是一个指数族分布。对隐变量和观测变量作出一定的限制，就得到了受限玻尔兹曼机（RBM）。

不同的概率图模型对下面几个特点作出假设：

1. 方向 - 边的性质
2. 离散/连续/混合 - 点的性质
3. 条件独立性 - 边的性质
4. 隐变量 - 节点的性质
5. 指数族 - 结构特点

将观测变量和隐变量分别记为$v, h, h = \{h_1, ⋯, h_m\}, v= \{v_1, ⋯,v_n\}$。我们知道，无向图根据最大团的分解，可以写为玻尔兹曼分布的形式 $p(x) = \frac{1}{Z} ∏_{i=1}^kψ_i(x_{ci}) = \frac{1}{Z}\exp(-∑_{i=1}^kE(x_{ci}))$，这也是一个指数族分布。

一个玻尔兹曼机存在一系列的问题，在其推断任务中，想要精确推断，是无法进行的，想要近似推断，计算量过大。为了解决这个问题，一种简化的玻尔兹曼机 - 受限玻尔兹曼机作出了假设，所有隐变量内部以及观测变量内部没有连接，只在隐变量和观测变量之间有连接，这样一来：

$$
p(x) = p(h,v) = \frac{1}{Z}\exp(-E(v, h))
$$

其中能量函数 $E(v, h)$可以写出三个部分，包括与节点集合相关的两项以及与边$w$相关的一项，记为：

$$
E(v, h)=-(h^{⊤} wv+α^{⊤} v + β^{⊤} h)
$$

故：

$$
p(x) = \frac{1}{Z}\exp(h^{⊤} wv)\exp(α^{⊤} v)\exp(β^{⊤} h) = \frac{1}{Z} ∏_{i=1}^m∏_{j=1}^n\exp(h_iw_{ij} v_j)∏_{j=1}^n\exp(α_j v_j)∏_{i=1}^m\exp(β_ih_i)
$$

上面这个式子也和 RBM 的因子图一一对应。

## 1. 推断

推断任务包括求后验概率$p(v|h), p(h|v)$以及求边缘概率$p(v)$。

### 1.1. $p(h|v)$

对于一个无向图，满足局域的Markov性质，即$p(h_1|h-\{h_1\},v) = p(h_1|Neighbour(h_1)) = p(h_1|v)$。我们可以得到：

$$
p(h|v)= ∏_{i=1}^mp(h_i|v)
$$

考虑 Binary RBM，所有的隐变量只有两个取值$0, 1$：

$$
p(h_l=1|v) = \frac{p(h_l=1, h_{-l},v)}{p(h_{-l},v)} = \frac{p(h_l=1, h_{-l},v)}{p(h_l=1, h_{-l},v) + p(h_l=0, h_{-l},v)}
$$

将能量函数写成和 $l$相关或不相关的两项：

$$
E(v, h) = -(∑_{i=1,i≠l}^m∑_{j=1}^nh_iw_{ij} v_j + h_l∑_{j=1}^nw_{lj} v_j+ ∑_{j=1}^nα_j  v_j + ∑_{i=1,i≠l}^mβ_ih_i + β_lh_l)
$$

**定义**：$h_lH_l(v)=h_l∑_{j=1}^nw_{lj} v_j + β_lh_l,\overline{H}(h_{-l},v) = ∑_{i=1,i≠l}^m∑_{j=1}^nh_iw_{ij} v_j+ ∑_{j=1}^nα_j  v_j+ ∑_{i=1,i≠l}^mβ_ih_i$。

代入，有：

$$
p(h_l=1|v) = \frac{\exp(H_l(v) + \overline{H}(h_{-l}, v))}{\exp(H_l(v) + \overline{H}(h_{-l},v)) + \exp(\overline{H}(h_{-l},v))} = \frac{1}{1 + \exp(-H_l(v))} = σ(H_l(v))
$$

于是就得到了后验概率。对于$v$的后验是对称的，故类似的可以求解。

### 1.2. $p(v)$

$$
\begin{aligned}p(v) &= ∑_hp(h,v) = ∑_h\frac{1}{Z}\exp(h^{⊤} wv+α^{⊤}v + β^{⊤}h) \\
&= \exp(α^{⊤}v)\frac{1}{Z} ∑_{h_1}\exp(h_1w_1v + β_1h_1)⋯∑_{h_m}\exp(h_mw_mv + β_mh_m) \\
&= \exp(α^{⊤}v)\frac{1}{Z}(1 + \exp(w_1v + β_1))⋯(1 + \exp(w_mv + β_m)) \\
&= \frac{1}{Z}\exp(α^{⊤}v+ ∑_{i=1}^m\log(1 + \exp(w_iv + β_i)))
\end{aligned}
$$

其中，$\log(1 + \exp(x))$称为 Softplus 函数。

## 2. RBM 的学习问题

RBM 的参数为：

$$
\begin{aligned}
h=(h_1, ⋯, h_m)^{⊤} \\
v=(v_1, ⋯,v_n)^{⊤} \\
w=(w_{ij})_{mn} \\
α=(α_1, ⋯,α_n)^{⊤} \\
β=(β_1, ⋯,β_m)^{⊤}
\end{aligned}
$$

学习问题关注的概率分布为：

$$
\begin{aligned}
\log p(v) &= \log∑_hp(h,v) \\
&= \log∑_h\frac{1}{Z}\exp(-E(v, h)) \\
&= \log∑_h\exp(-E(v, h))-\log∑_{v, h}\exp(-E(h,v))
\end{aligned}
$$

对上面这个式子求导第一项：

$$
\frac{∂\log∑_h\exp(-E(v, h))}{∂θ} = -\frac{∑_h\exp(-E(v, h))\frac{∂E(v, h)}{∂θ}}{∑_h\exp(-E(v, h))} \\
= -∑_h\frac{\exp(-E(v, h))\frac{∂E(v, h)}{∂θ}}{∑_h\exp(-E(v, h))} = -∑_hp(h|v)\frac{∂E(v, h)}{∂θ}
$$

第二项：

$$
\frac{∂\log∑_{v, h}\exp(-E(h,v))}{∂θ} = -∑_{h,v}\frac{\exp(-E(v, h))\frac{∂E(v, h)}{∂θ}}{∑_{h,v}\exp(-E(v, h))} =-∑_{v, h}p(v, h)\frac{∂E(v, h)}{∂θ}
$$

故有：

$$
\frac{∂}{∂θ}\log p(v) = -∑_hp(h|v)\frac{∂E(v, h)}{∂θ}+ ∑_{v, h}p(v, h)\frac{∂E(v, h)}{∂θ}
$$

将 RBM 的模型假设代入：

$$
E(v, h) = -(h^{⊤} wv+α^{⊤}v + β^{⊤}h)
$$

$w_{ij}$：

$$
\frac{∂}{∂w_{ij}}E(v, h) = -h_i v_j
$$

于是：

$$
\frac{∂}{∂θ}\log p(v) = ∑_hp(h|v) h_i v_j-∑_{h,v}p(h,v) h_i v_j
$$

第一项：

$$
∑_{h_1, h_2, ⋯, h_m}p(h_1, h_2, ⋯, h_m|v) h_i v_j= ∑_{h_i}p(h_i|v) h_i v_j = p(h_i = 1|v) v_j
$$

这里假设了 $h_i$是二元变量。

第二项：

$$
∑_{h,v}p(h,v) h_i v_j = ∑_{h,v}p(v)p(h|v) h_i v_j= ∑_vp(v)p(h_i = 1|v) v_j
$$

这个求和是指数阶的，于是需要采样解决，我么使用 CD-k 方法。

对于第一项，可以直接使用训练样本得到，第二项采用 CD-k 采样方法，首先使用样本$v⁰=v$，然后采样得到 $h⁰$，然后采样得到$v^1$，这样顺次进行，最终得到$v^{k}$，对于每个样本都得到一个$v^{k}$，最终采样得到 $N$个$v^{k} $，于是第二项就是：

$$
p(h_i = 1|v^{k}) v_j^{k}
$$

具体的算法为：

1. 对每一个样本中的$v$，进行采样：
2. 使用这个样本初始化采样
3. 进行 $k$次采样（0-k-1）：
4. $h_i^{l} ∼ p(h_i|v^{l})$
5. $v_i^{l+1} ∼ p(v_i|h^{l})$
6. 将这些采样出来的结果累加进梯度中
7. 重复进行上述过程，最终的梯度除以 $N$
