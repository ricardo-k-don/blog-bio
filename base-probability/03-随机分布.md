# 随机分布

## 1. 一维高斯分布

在 MLE 中

$$
\begin{aligned}
θ_\mathrm{MLE}
&= \underset{θ}{\mathrm{argmax}}\log p(X ∣ θ) \\
&= \underset{θ}{\mathrm{argmax}}\ ∑_{i=1}^N \log p(x_i ∣ θ)
\end{aligned}
$$

考虑一维高斯分布的概率密度函数（PDF）

$$
p(x ∣ μ, σ) = \frac{1}{(2π)^{1/2} σ} \exp[-\frac{1}{2 σ^2}(x - μ)^2]
$$

带入 MLE，有

$$
\begin{aligned}
\log p(X ∣ θ)
&= ∑_{i=1}^N\log\frac{1}{\sqrt{2π}σ}\exp(-(x_i - μ)^2/2σ^2) \\
&= ∑_{i=1}^N [\log\frac{1}{\sqrt{2π}} - \logσ - \frac{1}{2σ^2}(x_i - μ)^2]
\end{aligned}
$$

### 1.1. 参数估计

对$μ$的极值，有

$$
μ_\mathrm{MLE} = \underset{μ}{\mathrm{argmin}}\ ∑_{i=1}^N(x_i - μ)^2
$$

于是

$$
\frac{∂}{∂μ} ∑_{i=1}^N(x_i - μ)^2=0\\
↓\\
μ_\mathrm{MLE} = \frac{1}{N} ∑_{i=1}^N x_i
$$

对$σ$的极值，有

$$
\begin{aligned}
σ_\mathrm{MLE}
&= \underset{σ}{\mathrm{argmin}}\ ∑_{i=1}^N [-\log σ - \frac{1}{2σ^2}(x_i - μ)^2] \\
&= \underset{σ}{\mathrm{argmax}}\ ∑_{i=1}^N [\log σ + \frac{1}{2σ^2}(x_i - μ)^2]
\end{aligned}
$$

于是

$$
\frac{∂}{∂σ} ∑_{i=1}^N [\log σ+\frac{1}{2σ^2}(x_i - μ)^2]=0\\
↓\\
σ_\mathrm{MLE}^2= \frac{1}{N} ∑_{i=1}^N(x_i - μ)^2
$$

### 1.2. 无偏估计

值得注意的是，上面的推导中，首先对$μ$求 MLE，然后利用这个结果求$σ_\mathrm{MLE}$，因此可以预期的是对数据集求期望时$E[μ_\mathrm{MLE}]$是无偏差的：

$$
E[μ_\mathrm{MLE}]=E[\frac{1}{N} ∑_{i=1}^N x_i] = \frac{1}{N} ∑_{i=1}^NE\big[x_i\big] = μ
$$

但是当对$σ_\mathrm{MLE}$求 期望的时候由于使用了单个数据集的$μ_\mathrm{MLE}$，因此对所有数据集求期望的时候我们会发现$σ_\mathrm{MLE}$是有偏的：

$$
\begin{aligned}
E[σ_\mathrm{MLE}^2]
&= \frac{1}{N} ∑_{i=1}^NE[(x_i - μ_\mathrm{MLE})^2] \\
&= \frac{1}{N} ∑_{i=1}^NE[(x_i^2 - 2x_iμ_\mathrm{MLE} + μ_\mathrm{MLE}^2)] \\
&= \frac{1}{N} ∑_{i=1}^NE[x_i^2 - μ_\mathrm{MLE}^2] \\
&= \frac{1}{N} ∑_{i=1}^NE[x_i^2 - μ^2 + μ^2 - μ_\mathrm{MLE}^2] \\
&= \frac{1}{N} ∑_{i=1}^N\bigg\{E\big[x_i^2 - μ^2\big]-E[μ_\mathrm{MLE}^2 - μ^2]\bigg\}
\end{aligned}
$$

由方差性质，$D(X) = E(X^2) - E^2(X)$，有

$$
\begin{aligned}
E\big[x_i^2 - μ^2\big] &= D(X) + E^2(X) = σ^2 + μ^2 - μ^2 = σ^2 \\
E[μ_\mathrm{MLE}^2] - E^2[μ_\mathrm{MLE}] &= E(X^2) - E^2(X) = D[μ_\mathrm{MLE}]
\end{aligned}
$$

于是

$$
\begin{aligned}
E[σ_\mathrm{MLE}^2]
&= σ^2 - (E[μ_\mathrm{MLE}^2] - E^2[μ_\mathrm{MLE}]) \\
&= σ^2 - D[μ_\mathrm{MLE}] \\
&= σ^2 - D[\frac{1}{N} ∑_{i=1}^N x_i] \\
&= σ^2 - \frac{1}{N^2} ∑_{i=1}^ND\big[x_i\big] \\
& = \frac{N -1}{N}σ^2
\end{aligned}
$$

故，$σ^2$的无偏估计为

$$
σ̂^2= \frac{1}{N -1} ∑_{i=1}^N(x_i - μ)^2
$$

## 2. 多维高斯分布

考虑多维高斯分布的概率密度函数（PDF）

$$
p(x ∣ μ, Σ) = \frac{1}{(2π)^{p/2}|Σ|^{1/2}}\exp[-\frac{1}{2}\underbrace{(x- μ)^{⊤} Σ^{-1}(x- μ)}_{二次型}]
$$

其中，$x, μ ∈ ℝ^p$，$Σ ∈ ℝ^{p× p}$，即

$$
x=
\begin{bmatrix}
x_1 \\ x_2 \\⋮ \\ x_{p}
\end{bmatrix}
μ=
\begin{bmatrix}
μ_1 \\ μ_2 \\ ⋮ \\ μ_{p}
\end{bmatrix}
Σ=\begin{bmatrix}
σ_{11} & σ_{12} & ⋯ & σ_{1p} \\
σ_{21} & σ_{22} & ⋯ & σ_{2p} \\
⋮ & ⋮ & & ⋮ \\
σ_{p1} & σ_{p2} & ⋯ & σ_{pp}
\end{bmatrix}_{p× p}
$$

通常来说，协方差矩阵$Σ$是半正定的（对称的）。这里假设$Σ$是正定的（特征值$λ>0$），便于叙述。

### 2.1. 马氏距离

马氏距离（Mahalanobis distance）是一种距离的度量，可以看作是欧氏距离的一种修正，修正了欧式距离中各个维度尺度不一致且相关的问题。

$$
D\_{M}(x)=\sqrt{(x - μ)^{⊤} Σ^{-1}(x - μ)}
$$

其中，$Σ$是多维随机变量的协方差矩阵，$μ$为样本均值，若$Σ = 𝑰$，也就是各维度独立同分布，马氏距离就变成了欧氏距离。简言之，马氏距离是旋转变换缩放之后的欧式距离。

> 因为涉及求逆，$Σ$必须满秩。

### 2.2. 分解

显然，指数上的数字可以记为$x$和$μ$之间的马氏距离。对于对称的协方差矩阵可进行特征值分解

$$
Σ = 𝑼Λ𝑼^{⊤} =
(u*1,u_2, ⋯,u_p)
\mathrm{diag}(λ_i)(u_1,u_2, ⋯,u_p)^{⊤} =
∑*{i=1}^pu_iλ_iu_i^{⊤}
$$

于是

$$
Σ^{-1} = ∑\_{i=1}^pu_i\frac{1}{λ_i}u_i^{⊤}
$$

$$
Δ = (x- μ)^{⊤} Σ^{-1}(x- μ) = ∑\_{i=1}^p (x- μ)^{⊤} u_i \frac{1}{λ_i} u_i^{⊤}(x- μ)
$$

令$y_i = (x- μ)^{⊤} u_i$，得

$$
Δ = ∑\_{i=1}^p\frac{y_i^2}{λ_i}
$$

注意到，$y_i$为新的坐标轴，$y_i$是$x- μ$在特征向量$u_i$上的投影长度，因此上式就是$Δ$取不同值时的同心椭圆。

- 参数$Σ, μ$的自由度为$O(p^2)$对于维度很高的数据其自由度太高。
  - 因子分析
  - p-PCA
- 单个高斯分布是单峰的，对有多个峰的数据分布不能得到好的结果。
  - 高斯混合模型（GMM）

### 2.3. 边缘和条件概率

已知$x ∼ N(μ, σ)$，记

-$x = (x_1, x_2, ⋯, x_p)^{⊤} = (x_{a, m× 1}, x_{b, n×1})^{⊤}$
-$μ = (μ_{a,m×1}, μ_{b,n×1})$
-$Σ = \begin{bmatrix}Σ_{aa} &Σ_{ab} \\ Σ_{ba} &Σ_{bb}\end{bmatrix}$

首先是一个高斯分布的定理：

:::{admonition} 定理

已知$x ∼ N(μ, σ)$，$y ∼ 𝑨𝒙 + b$，则

$$
y ∼ N(𝑨μ + b, 𝑨𝜮𝑨^{⊤})
$$

证明

- $E\big[y\big] = E\big[𝑨𝒙 + b\big]=𝑨E\big[x\big] + b = 𝑨μ + b$
- $D\big[y\big] = D\big[𝑨𝒙 + b\big]=D\big[𝑨𝒙\big] = 𝑨⋅D\big[x\big]⋅𝑨^{⊤}$

:::

令

$$
x*a =
\underbrace{
\begin{bmatrix}
  𝑰*{m× m} & 𝑶*{m× n}
\end{bmatrix}}*{𝑨}
\underbrace{
\begin{bmatrix}
  x*a \\ x_b
\end{bmatrix}
}
*{x}-\underbrace{0}_B
$$

代入定理，得

$$
E\big[x_a\big]=
\begin{bmatrix}
  𝑰 & 𝑶
\end{bmatrix}
\begin{bmatrix}
μ_a \\μ_b
\end{bmatrix}
= μ_a \\
D\big[x_a\big]=
\begin{bmatrix}
  𝑰 & 𝑶
\end{bmatrix}
\begin{bmatrix}
Σ_{aa} &Σ_{ab} \\
Σ_{ba} &Σ_{bb}
\end{bmatrix}
\begin{bmatrix}
  𝑰 \\ 𝑶
\end{bmatrix}
=Σ_{aa}
$$

故$x_a ∼ N(μ_a, Σ_{aa})$。同理，$x_b ∼ N(μ_b, Σ_{bb})$。

引入三个量：

$$
\begin{aligned}
  x_{b⋅a} &= x_b - Σ_{ba} Σ_{aa}^{-1} x_a \\
  μ_{b⋅a} &= μ_b - Σ_{ba} Σ_{aa}^{-1}μ_a \\
  Σ_{bb⋅a} &= Σ_{bb} - Σ_{ba} Σ_{aa}^{-1} Σ_{ab}
\end{aligned}
$$

> $Σ_{bb⋅a}$称为$Σ_{bb}$的 Schur Complementary。
> $x_{b⋅a}$是$x_b$与$x_a$的线性组合，故其服从高斯分布。

对$x_{b⋅a}$进行变换，得

$$
x_{b⋅a} =
\begin{bmatrix}
  -Σ_{ba} Σ_{aa}^{-1} & 𝑰_{n× n}
\end{bmatrix}
\begin{bmatrix}
  x_a \\ x_b
\end{bmatrix}
$$

故：

$$
E\big[x_{b⋅a}\big]=
\begin{bmatrix}
  -Σ_{ba} Σ_{aa}^{-1} &𝑰_{n× n}
\end{bmatrix}
\begin{bmatrix}
  μ_a \\μ_b
\end{bmatrix}
= μ_{b⋅a} \\
D\big[x_{b⋅a}\big]=
\begin{bmatrix}
  -Σ_{ba} Σ_{aa}^{-1} &𝑰_{n× n}
\end{bmatrix}
\begin{bmatrix}
  Σ_{aa} &Σ_{ab} \\
  Σ_{ba} &Σ_{bb}
\end{bmatrix}
\begin{bmatrix}
  -Σ_{aa}^{-1} Σ_{ba}^{⊤} \\
  𝑰_{n× n}
\end{bmatrix}
= Σ_{bb⋅a}
$$

故，$x_{b⋅a} ∼ N(μ_{b⋅a}, Σ_{bb⋅a})$

利用这三个量可以得到

$$
x_b = x_{b⋅a} + Σ_{ba} Σ_{aa}^{-1} x_a
$$

可得

$$
E\big[x_b ∣ x_a\big] = μ_{b⋅a}+Σ_{ba} Σ_{aa}^{-1} x_a
$$

$$
D\big[x_b ∣ x_a\big] = Σ_{bb⋅a}
$$

这里同样用到了定理。

$$
x_{a⋅b} =x_a-Σ_{ab} Σ_{bb}^{-1} x_b\\
μ_{a⋅b} = μ_a-Σ_{ab} Σ_{bb}^{-1}μ_b\\
Σ_{aa⋅b} =Σ_{aa}-Σ_{ab} Σ_{bb}^{-1} Σ_{ba}
$$

故

$$
E\big[x_a ∣ x_b\big] = μ_{a⋅b}+Σ_{ab} Σ_{bb}^{-1} x_b
$$

$$
D\big[x_a ∣ x_b\big]=Σ_{aa⋅b}
$$

因此

$$
x_a ∣ x_b ∼ N(μ_{a⋅b}+Σ_{ab} Σ_{bb}^{-1} x_b, Σ_{aa⋅b})
$$

### 2.4. 联合概率分布

已知$p(x) = N(μ, Λ^{-1}), p(y ∣ x) = N(𝑨𝒙 + b, L^{-1})$，求$p(y), p(x ∣ y)$。

> $Λ^{-1}$称为精确矩阵（precision matrix），是协方差矩阵的逆矩阵。

令

$$
y = 𝑨𝒙 + b + ϵ,\ ϵ ∼ N(0, L^{-1})
$$

故

- $E\big[y\big] = E\big[𝑨𝒙 + b + ϵ\big] = 𝑨μ + b$
- $D\big[y\big] = D\big[𝑨𝒙 + b + ϵ\big] = 𝑨D𝑨^{⊤} + D\big[ϵ\big] = 𝑨Λ^{-1}𝑨^{⊤} + L^{-1}$

因此

$$
y ∼ N(𝑨μ + b, L^{-1} + 𝑨Λ^{-1}𝑨^{⊤})
$$

引入

$$
z =
\begin{bmatrix}
x\\ y
\end{bmatrix}
$$

可得

$$
\begin{aligned}
Cov(x,y)
&= E\big[(x-E\big[x\big])(y-E\big[y\big])^{⊤}\big] \\
&= E\big[(x- μ)(𝑨𝒙-𝑨μ +  ϵ)^{⊤}\big] \\
&= E\big[(x- μ)(x- μ)^{⊤}𝑨^{⊤}\big]=D\big[x\big]𝑨^{⊤} \\
&= Λ^{-1}𝑨^{⊤}
\end{aligned}
$$

由协方差矩阵的对称性，有$Cov(y, x) = 𝑨^{⊤}Λ^{-1}$，则

$$
z ∼ N\bigg(
\begin{bmatrix}
μ \\ 𝑨μ + b
\end{bmatrix},
\begin{bmatrix}
Λ^{-1} &Λ^{-1}𝑨^{⊤} \\ 𝑨Λ^{-1} &L^{-1} + 𝑨Λ^{-1}𝑨^{⊤}
\end{bmatrix}
\bigg)
$$

根据之前的结论，$x_a ∣ x_b ∼ N(μ_{a⋅b}+Σ_{ab} Σ_{bb}^{-1} x_b, Σ_{aa⋅b})$，可得

$$
E\big[x ∣ y\big] = μ + Λ^{-1}𝑨^{⊤}(L^{-1} + 𝑨Λ^{-1}𝑨^{⊤})^{-1}(y-𝑨μ-b)
$$

$$
D\big[x ∣ y\big] = Λ^{-1}-Λ^{-1}𝑨^{⊤}(L^{-1} + 𝑨Λ^{-1}𝑨^{⊤})^{-1}𝑨Λ^{-1}
$$

因此

$$
x ∣ y ∼ N(μ + Λ^{-1} 𝑨^{⊤}(L^{-1} + 𝑨 Λ^{-1} 𝑨^{⊤})^{-1}(y - 𝑨μ - b), Λ^{-1}-Λ^{-1} 𝑨^{⊤}(L^{-1} + 𝑨 Λ^{-1} 𝑨^{⊤})^{-1} 𝑨 Λ^{-1})
$$

## 3. Beta 分布

意义：一个概率的概率分布（取值$\big[0, 1\big]$）

$$
f(x, α, β) = \frac{1}{Β(α, β)} x^{α-1} (1 - x)^{β -1}
$$

其中，$α > 0, β > 0$

## 4. Gamma分布

意义：随机变量$X$为等到第$α$件事$X_i$发生所需等候时间$β$

$$
f(x, α, β) =
\begin{cases}
  \dfrac{β^{α} x^{α-1}}{Γ(α)}e^{-βx}, & x ≥ 0\\
  0, & x < 0\\
\end{cases}
$$

其中，$α$为形状参数，$1/β$为标度参数（scale parameter）。

### 4.1. 性质

- 等标度可加性：$X ∼ Ga(α_1, β), Y ∼ Ga(α_2, β) ⇒ (X + Y) ∼ Ga(α_1 + α_2, β)$
-$Ga(1) ∼ Exp(λ)$
-$Ga(\frac{n}{2}, 2) ∼ χ^2(n) ⇒ Ga(\frac{1}{2}, 2) ∼ N(0, 1)$
