# 线性模型补充

## 1. 贝叶斯线性回归

我们知道，线性回归当噪声为高斯分布的时候，最小二乘损失导出的结果相当于对概率模型应用 MLE，引入参数的先验时，先验分布是高斯分布，则 MAP 的结果相当于岭回归的正则化，若先验是 LaPlace 分布，则相当于 Lasso 的正则化。这两种方案都是点估计方法。我们希望利用贝叶斯方法来求解参数的后验分布。

线性回归的模型假设为：

$$
\begin{aligned}
f(x)=w^{⊤} x\\
y=f(x)+ɛ\\
ɛ ∼ N(0, σ^2)
\end{aligned}
$$

引入高斯先验：

$$
p(w)=N(0, Σ_p)
$$

对参数的后验分布进行推断：

$$
p(w|X,Y) = \frac{p(w,Y|X)}{p(Y|X)} = \frac{p(Y|w, X)p(w|X)}{∫ p(Y|w, X)p(w|X)dw}
$$

分母和参数无关，由于 $p(w|X)=p(w)$，代入先验得到：

$$
p(w|X,Y)∝ ∏_{i=1}^nN(y_i|w^{⊤} x_i, σ^2)⋅N(0, Σ_p)
$$

高斯分布取高斯先验的共轭分布依然是高斯分布，于是可以得到后验分布也是一个高斯分布。第一项：

$$
\begin{aligned}
∏_{i=1}^nN(y_i|w^{⊤} x_i, σ^2) &= \frac{1}{(2π)^{N/2}σ^n}\exp(-\frac{1}{2σ^2} ∑_{i=1}^n(y_i - w^{⊤} x_i)^2) \\
&= \frac{1}{(2π)^{N/2}σ^n}\exp(-\frac{1}{2}(Y- Xw)^{⊤}(σ^{-2}𝑰)(Y- Xw))
\\&=N(Xw, σ^2𝑰)
\end{aligned}
$$

代入上面的式子：

$$
p(w|X,Y)∝\exp\bigg(-\frac{1}{2σ^2}(Y- Xw)^{⊤}σ^{-2}𝑰(Y- Xw)-\frac{1}{2} w^{⊤} Σ_p^{-1} w\bigg)
$$

假设最后得到的高斯分布为：$N(μ_w, Σ_w)$。对于上面的分布，采用配方的方式来得到最终的分布，指数上面的二次项为：

$$
-\frac{1}{2σ^2} w^{⊤} 𝑿^{⊤} Xw -\frac{1}{2} w^{⊤} Σ_p^{-1} w
$$

于是：

$$
Σ_w^{-1} =σ^{-2}𝑿^{⊤} X+Σ_p^{-1} =A
$$

一次项：

$$
\frac{1}{2σ^2}2Y^{⊤} Xw=σ^{-2}Y^{⊤} Xw
$$

于是：

$$
μ_w^{⊤} Σ_w^{-1} =σ^{-2}Y^{⊤} X⇒μ_w=σ^{-2}𝑨^{-1}𝑿^{⊤} Y
$$

给定一个 $x^{*}$，求解 $y^{*}$，故 $f(x^{*})=x^{*T} w$，代入参数后验，有 $x^{*T} w∼ N(x^{*T}μ_w, x^{*T} Σ_wx^{*})$，添上噪声项：

$$
p(y^{*}|X,Y, x^{*}) = ∫_wp(y^{*}|w, X,Y, x^{*})p(w|X,Y, x^{*})dw= ∫_wp(y^{*}|w, x^{*})p(w|X,Y)dw\\
=N(x^{*T}μ_w, x^{*T} Σ_wx^{*}+ σ^2)
$$

## 2. 核贝叶斯线性回归

贝叶斯线性回归可以通过加入核函数的方法来解决非线性函数的问题，将 $f(x)=x^{⊤} w$ 这个函数变为 $f(x)=ϕ(x)^{⊤} w$（当然这个时候，$Σ_p$ 也要变为更高维度的），变换到更高维的空间，有：

$$
\begin{aligned}
f(x^{*})∼ N(ϕ(x^{*})^{⊤}σ^{-2}𝑨^{-1}Φ^{⊤} Y,ϕ(x^{*})^{⊤}𝑨^{-1}ϕ(x^{*})) \\
A=σ^{-2}Φ^{⊤}Φ+Σ_p^{-1}
\end{aligned}
$$

其中，$Φ=(ϕ(x_1),ϕ(x_2), ⋯,ϕ(x_N))^{⊤}$。

为了求解 $𝑨^{-1}$，可以利用 Woodbury Formula，$A=Σ_p^{-1},C=σ^{-2}𝑰$：

$$
(A+UCV)^{-1} =𝑨^{-1}-𝑨^{-1}U(C^{-1}+V𝑨^{-1}U)^{-1}V𝑨^{-1}
$$

故 $𝑨^{-1} =Σ_p-Σ_pΦ^{⊤}(σ^2𝑰+ΦΣ_pΦ^{⊤})^{-1}ΦΣ_p$

也可以用另一种方法：

$$
\begin{aligned}
A&=σ^{-2}Φ^{⊤}Φ+Σ_p^{-1} \\
⇔ AΣ_p&=σ^{-2}Φ^{⊤}ΦΣ_p+𝑰\\
⇔ AΣ_pΦ^{⊤}&=σ^{-2}Φ^{⊤}ΦΣ_pΦ^{⊤} +Φ^{⊤}=σ^{-2}Φ^{⊤}(k+ σ^2𝑰) \\
⇔ Σ_pΦ^{⊤}&=σ^{-2}𝑨^{-1}Φ^{⊤}(k+ σ^2𝑰) \\
⇔ σ^{-2}𝑨^{-1}Φ^{⊤}&=Σ_pΦ^{⊤}(k+ σ^2𝑰)^{-1} \\
⇔ ϕ(x^{*})^{⊤}σ^{-2}𝑨^{-1}Φ^{⊤}&=ϕ(x^{*})^{⊤} Σ_pΦ^{⊤}(k+ σ^2𝑰)^{-1}
\end{aligned}
$$

上面的左边的式子就是变换后的均值，而右边的式子就是不含 $𝑨^{-1}$ 的式子，其中 $k=ΦΣ_pΦ^{⊤}$。

根据 $𝑨^{-1}$ 得到方差为：

$$
ϕ(x^{*})^{⊤} Σ_pϕ(x^{*})-ϕ(x^{*})^{⊤} Σ_pΦ^{⊤}(σ^2𝑰+k)^{-1}ΦΣ_pϕ(x^{*})
$$

上面定义了：

$$
k=ΦΣ_pΦ^{⊤}
$$

我们看到，在均值和方差中，含有下面四项：

$$
ϕ(x^{*})^{⊤} Σ_pΦ^{⊤},ϕ(x^{*})^{⊤} Σ_pϕ(x^{*}),ϕ(x^{*})^{⊤} Σ_pΦ^{⊤},ΦΣ_pϕ(x^{*})
$$

展开后，可以看到，有共同的项：$k(x, x^{′})=ϕ(x)^{⊤} Σ_pϕ(x′)$。由于 $Σ_p$ 是正定对称的方差矩阵，故，这是一个核函数。

对于高斯过程中的协方差：

$$
k(t,s) = Cov\big[f(x),f(x^{′})\big] = \mathrm{E}\big[ϕ(x)^{⊤} ww^{⊤}ϕ(x^{′})\big]=ϕ(x)^{⊤}\mathrm{E}\big[ww^{⊤}\big]ϕ(x^{′})=ϕ(x)^{⊤} Σ_pϕ(x^{′})
$$

我们可以看到，这个就对应着上面的核函数。因此我们看到 $\{f(x)\}$ 组成的组合就是一个高斯过程。

## 3. 函数空间的观点

相比权重空间，我们也可以直接关注 $f$ 这个空间，对于预测任务，这就是类似于求：

$$
p(y^{*}|X,Y, x^{*})= ∫_fp(y^{*}|f, X,Y, x^{*})p(f|X,Y, x^{*})df
$$

对于数据集来说，取 $f(X) ∼ N(μ(X),k(X, X)),Y=f(X)+ɛ ∼ N(μ(X),k(X, X)+ σ^2𝑰)$。预测任务的目的是给定一个新数据序列 $X^{*}=(x_1^{*}, ⋯, x_M^{*})^{⊤}$，得到 $Y^{*}=f(X^{*})+ɛ$。我们可以写出：

$$
\begin{bmatrix}
Y\\f(X^{*})
\end{bmatrix}
 ∼ N
\bigg(\begin{bmatrix}
μ(X) \\μ(X^{*})
\end{bmatrix},
\begin{bmatrix}
k(X, X)+ σ^2𝑰&k(X, X^{*}) \\
k(X^{*}, X) &k(X^{*}, X^{*})
\end{bmatrix}\bigg)
$$

根据高斯分布的方法：

$$
\begin{aligned} x=
\begin{bmatrix}
x_a\\ x_b
\end{bmatrix}
 ∼ N
\bigg(\begin{bmatrix}
μ_a\\μ_b
\end{bmatrix},
\begin{bmatrix}
Σ_{aa} &Σ_{ab} \\
Σ_{ba} &Σ_{bb}
\end{bmatrix}\bigg) \\
x_b ∣ x_a ∼ N(μ_{b|a}, Σ_{b|a}) \\
μ_{b|a} =Σ_{ba} Σ_{aa}^{-1}(x_a-μ_a)+μ_b\\
Σ_{b|a} =Σ_{bb}-Σ_{ba} Σ_{aa}^{-1} Σ_{ab}
\end{aligned}
$$

可以直接写出：

$$
\begin{cases}
  \begin{aligned}
  p(f(X^{*})|X,Y, X^{*})
  &= N(k(X^{*}, X) \big[k(X, X)+ σ^2𝑰\big]^{-1}(Y-μ(X))+μ(X^{*})\\
  &=p(f(X^{*})|Y)
  \end{aligned}\\
k(X^{*}, X^{*}) - k(X^{*}, X) \big[k(X, X) + σ^2𝑰\big]^1 k(X, X^{*}))
\end{cases}
$$

故对于 $Y = f(X^{*}) + ɛ$：

$$
N(k(X^{*}, X) \big[k(X, X)+ σ^2𝑰\big]^{-1}(Y-μ(X))+μ(X^{*}),\\
k(X^{*}, X^{*}) - k(X^{*}, X) \big[k(X, X)+ σ^2𝑰\big]^1 k(X, X^{*})+ σ^2𝑰)
$$

我们看到，函数空间的观点更加简单易于求解。

## 4. 高斯判别分析

生成模型中，我们对联合概率分布进行建模，然后采用 MAP 来获得参数的最佳值。两分类的情况，我们采用的假设：

$$
\begin{aligned}
  y &∼ \mathrm{Bern}(ϕ) \\
  x ∣ y=1 &∼ N(μ_1, Σ) \\
  x ∣ y=0 &∼ N(μ_0, Σ)
\end{aligned}
$$

则独立全同的数据集最大后验概率可以表示为

$$
\begin{aligned}
\underset{ϕ, μ*0, μ_1, Σ}{\mathrm{argmax}}\ \log p(X|Y)p(Y)
&= \underset{ϕ, μ_0, μ_1, Σ}{\mathrm{argmax}}\ ∑_{i=1}^n (\log p(x*i|y_i)+\log p(y_i)) \\
&= \underset{ϕ, μ_0, μ_1, Σ}{\mathrm{argmax}}\ ∑_{i=1}^n((1 - y_i)\log N(μ_0, Σ)+y_i\log N(μ_1, Σ)+y_i\logϕ+(1 - y_i)\log(1-ϕ))
\end{aligned}
$$

- 首先对$ϕ$进行求解，将式子对$ϕ$求偏导

$$
∑_{i=1}^n\frac{y_i}{ϕ}+\frac{y_i -1}{1-ϕ} =0 \\
↓ \\
ϕ = \frac{∑_{i=1}^n y_i}{N} = \frac{N_1}{N}
$$

- 然后求解 $μ_1$

$$
\begin{aligned}
\hat{μ}
&= \underset{μ*1}{\mathrm{argmax}}\ ∑_{i=1}^n y*i\log N(μ_1, Σ) \\
&= \underset{μ_1}{\mathrm{argmin}}\ ∑_{i=1}^n y_i(x_i -μ_1)^{⊤} Σ^{-1}(x_i -μ_1)
\end{aligned}
$$

由

$$
∑_{i=1}^n y_i(x_i -μ_1)^{⊤} Σ^{-1}(x_i -μ_1) = ∑_{i=1}^n y_ix_i^{⊤} Σ^{-1} x_i -2y_iμ_1^{⊤} Σ^{-1} x_i+y_iμ_1^{⊤} Σ^{-1}μ_1
$$

求微分左边乘以$Σ$可得

$$
∑_{i=1}^n -2y_iΣ^{-1} x_i+2y_iΣ^{-1}μ_1=0 \\
↓ \\
μ_1= \frac{∑_{i=1}^n y*ix_i}{∑_{i=1}^n y*i} = \frac{∑_{i=1}^n y_ix_i}{N_1}
$$

- 求解 $μ_0$，由于正反例是对称的，故：

$$
μ_0= \frac{∑_{i=1}^n(1 - y_i)x_i}{N_0}
$$

最为困难的是求解 $Σ$，我们的模型假设对正反例采用相同的协方差矩阵，当然从上面的求解中我们可以看到，即使采用不同的矩阵也不会影响之前的三个参数。

首先我们有：

$$
\begin{aligned}
∑_{i=1}^n\log N(μ, σ) &= ∑_{i=1}^n\log(\frac{1}{(2π)^{p/2}|Σ|^{1/2}})+(-\frac{1}{2}(x_i -μ)^{⊤} Σ^{-1}(x_i -μ)) \\
&=\mathrm{Const}-\frac{1}{2} N\log|Σ|-\frac{1}{2}\mathrm{tr}((x_i -μ)^{⊤} Σ^{-1}(x_i -μ)) \\
&=\mathrm{Const}-\frac{1}{2} N\log|Σ|-\frac{1}{2}\mathrm{tr}((x_i -μ)(x_i -μ)^{⊤} Σ^{-1}) \\
&=\mathrm{Const}-\frac{1}{2} N\log|Σ|-\frac{1}{2} N\mathrm{tr}(SΣ^{-1})
\end{aligned}
$$

在这个表达式中，我们在标量上加入迹从而可以交换矩阵的顺序，对于包含绝对值和迹的表达式的导数，我们有：

$$
\begin{aligned}
\frac{∂}{∂𝑨}(|𝑨|) &= |𝑨|𝑨^{-1} \\
\frac{∂}{∂𝑨}\mathrm{tr}(𝑨𝑩) &= 𝑩^{⊤}
\end{aligned}
$$

因此

$$
∑_{i=1}^n [(1 - y_i)\log N(μ_0, Σ) + y_i \log N(μ_1, Σ)]^{′} = \mathrm{Const} - \frac{1}{2} N\log|Σ|-\frac{1}{2} N_1\mathrm{tr}(S_1Σ^{-1})-\frac{1}{2} N_2\mathrm{tr}(S_2Σ^{-1})
$$

其中，$S_1,S_2$ 分别为两个类数据内部的协方差矩阵，于是：

$$
N Σ^{-1} - N_1S_1^{⊤} Σ^{-2} - N_2S_2^{⊤} Σ^{-2} =0 \\
↓\\
Σ = \frac{N_1S_1 + N_2S_2}{N}
$$

这里应用了类协方差矩阵的对称性。于是我们就利用最大后验的方法求得了我们模型假设里面的所有参数，根据模型，可以得到联合分布，也就可以得到用于推断的条件分布了。

## 5. 朴素贝叶斯

上面的高斯判别分析的是对数据集的分布作出了高斯分布的假设，同时引入伯努利分布作为类先验，从而利用最大后验求得这些假设中的参数。

朴素贝叶斯队数据的属性之间的关系作出了假设，一般地，我们有需要得到 $p(x ∣ y)$ 这个概率值，由于 $x$ 有 $p$ 个维度，因此需要对这么多的维度的联合概率进行采样，但是我们知道这么高维度的空间中采样需要的样本数量非常大才能获得较为准确的概率近似。

在一般的有向概率图模型中，对各个属性维度之间的条件独立关系作出了不同的假设，其中最为简单的一个假设就是在朴素贝叶斯模型描述中的条件独立性假设。

$$
p(x ∣ y)= ∏_{i=1}^pp(x_i|y)
$$

即：

$$
x_i ⟂ x_{j}|y,∀\ i≠j
$$

于是利用 Bayes 定理，对于单次观测：

$$
p(y ∣ x) = \frac{p(x ∣ y)p(y)}{p(x)} = \frac{∏_{i=1}^pp(x_i|y)p(y)}{p(x)}
$$

对于单个维度的条件概率以及类先验作出进一步的假设：

1. $x_i$ 为连续变量：$p(x_i|y)=N(μ_i, σ_i^2)$
2. $x_i$ 为离散变量：$p(x_i=i|y)=θ_i,∑_{i=1}^kθ_i = 1$
3. $p(y)=ϕ^y(1-ϕ)^{1 - y}$

对这些参数的估计，常用 MLE 的方法直接在数据集上估计，由于不需要知道各个维度之间的关系，因此，所需数据量大大减少了。估算完这些参数，再代入 Bayes 定理中得到类别的后验分布。
